#!/usr/bin/env python3

import ast 
import subprocess
import glob
import argparse 
import os
import operator
from collections import Counter
from ete3 import NCBITaxa
import timeit
from collections import defaultdict
ncbi = NCBITaxa()

print("AAAAHHHHASDLKFJASLKFJLAKSDJFLAKSJFLKJ")
# Make a function to run a shell command and catch any errors
def subprocess_call(cmd):
    return_code = subprocess.call(cmd, shell = True)
    assert return_code == 0, "Exit code %d for %s" % (
        return_code,
        cmd
    )


# The tie-breaking function takes a list of taxid,edit_distances in the form [[taxid,edit_distance],[taxid,edit_distance],..
# references the global variable LOGIC to control the underlying tiebreaking code, breaks ties and 
# returns a single taxid as a string and a Boolean value (as to whether the read needs to re-BLASTEd against host).
def tie_break(taxid_list):
	score_list = [] 
	actual_taxid_list = []
	for id in taxid_list:
		score_list.append(id[1])
	
	# this can filter out any hits that are sufficiently worse than the edit distance for the 'best'
	# alignment - set to zero to only hold scores that have the best edit distance , and increase to a 
	# number greater than the snap option -d to hold all hits
	best_edit_distance = min(score_list) + 6
	
	# Keep taxids that have an edit distance less than the acceptable edit distance defined above 
	for id in taxid_list:
		if id[1] <= best_edit_distance:
			actual_taxid_list.append(id[0])
	#No longer holding edit distances		
	taxid_list = actual_taxid_list
	lineage_list = []
	
	for id in taxid_list:
		# Not all taxids have valid lineages 
		try:
			#Not every taxid has a valid NCBI lineage, so this statement has to be encased in a try statement.
			lineage = ncbi.get_lineage(id)
			# filters out lineages that contain taxids in the FILTER_LIST variable
			# commonly, this is 'other sequences', 'artificial sequences' or 'environmental samples' 
			if any(x in [12908,28384,48479] for x in lineage):
				lineage = []
		except:
			lineage = []
		
		if lineage:
			lineage_list.append(lineage)
	
	if not lineage_list:
		return ['*',False]
	
	# controls if use any alignment to the human genome as grounds for classification as human source
	if "false" == "true":
		# check if H_TAXID ever shows up in 
		if any(int(9606) in sl for sl in lineage_list):
			return [9606,False]
			

	# count all taxids in all lineages 
	taxid_to_count_map = {}
	for each_lineage in lineage_list:
		for each_taxid in each_lineage:
			if each_taxid in taxid_to_count_map:
				taxid_to_count_map[each_taxid] += 1
			else:
				taxid_to_count_map[each_taxid] = 1
	
	#Set the threshold according to the pre-specified LOGIC in the initialization file
	num_assignments = len(lineage_list)
	if "strict" == 'strict':
		threshold = num_assignments
	elif "strict" == '90':
		threshold = num_assignments - ((num_assignments /10) + 1) 
	elif "strict" == 'oneoff':
		threshold = num_assignments - 1
	else:
		print('invalid logic threshold: defaulting to strict')
		threshold = num_assignments
	
	#Now we will find all the taxids that meet threshold/LOGIC specified above.
	surviving_taxids = []
	for taxid_key in taxid_to_count_map:
		# main filtering - everything that passes this step gets a list intersection and the 
		# most specific taxid left is returned 
		if taxid_to_count_map[taxid_key] >= threshold:
			surviving_taxids.append(taxid_key)
			
	if len(surviving_taxids) == 0:
		return ['*',False]

	d = {}

	for the_value in surviving_taxids:
		d[the_value] = len(ncbi.get_lineage(the_value))
	#Find the remaining taxid with the longest lineage.
	#The longest lineage is defined as the longest list.  #nicetohave: this is not pulling the taxonomic rank at any point.
	assigned_hit = max(d.items(), key=operator.itemgetter(1))[0]
	recheck = False 
	
	#Assign a Boolean value for each read as to whether it needs to be searched against a custom BLAST database
	#Here, we are just assigning whether it needs to be searched or not.  The custom BLAST database would need to be made separately.
	#All reads downstream of INCLUSION_TAXID and but not downstream of EXCLUSION_TAXID will be assigned a True value.
	if "false" == "true":
		assigned_lineage = ncbi.get_lineage(assigned_hit)
		if 2759 in assigned_lineage and 9604 not in assigned_lineage:
			recheck = True
	
	return [assigned_hit, recheck]


#Every read has a taxid assignment or is unassigned at this point.

# wrapper for a kraken script that converts tab seperated taxid	count file and writes a 
# Pavian output file for it. Requires a copy of ncbi's taxonomy database and some blank files
# This is a map of assigned taxids to number of occurrences as well as the total number of unassigned reads.
def new_write_kraken(basename, final_counts_map, num_unassigned):
	print('Preparing output for e_coli_2')
	# we write a file in the form taxid	count 
	l = open('e_coli_2_temp_kraken.tsv', 'w')
	
	# initialize with the number of unassigned, we'll need to add human host filtering in earlier
	# because some reads will get tie broken to human 
	
	l.write('0	' + str(num_unassigned))
	# write the rest of the taxids to the file
	for key in final_counts_map.keys():
		l.write('\n' + str(key) + '	' + str(final_counts_map[key]))
	
	# this close is required so we get an EOF before passing the file to kraken-report 
	l.close()
	
	# kraken-report creates a file that Pavian likes - we name the file base_final_report.tsv
	kraken_report_cmd = '/usr/local/miniconda/bin/krakenuniq-report --db kraken_db --taxon-counts e_coli_2_temp_kraken.tsv > e_coli_2_final_report.tsv'
	subprocess_call(kraken_report_cmd)
	

# takes a list of finished output files and builds sam files for species level assignments 
# for each sample sam files are wrote directly to disk 
def build_sams(input_list):
	
	# only try to import this if we're trying to build sam files 
	from Bio import Entrez 
	Entrez.email = vpeddu@uw.edu
	
	# go through each file report output files 
	for file_name in glob.glob('*report.tsv'):
		base = file_name.split("_")[0]
		taxid_to_assemble = []
		
		# grab a list of taxids that are at a species level and also have greater than MIN_READ_CUTOFF assigned to them 
		for line in open(file_name):
			line_list = line.split('	')
			if line_list[3] == 'S' and int(line_lsit[2]) >= 10:
				lineage = ncbi.get_lineage(line_list[4])
				# filter out any taxids that have lineages that include anything from the blacklist
				if not any(x in "[2759,77133]" for x in lineage):
					taxid_to_assemble.append(line_list[4])
		
		# go through each taxid that we pulled in the last loop and parse the sam file for the accession numbers of the entries that each read aligned to
		# Each read can align to more than one NT entry across all SNAP databases so we grab every accession number that is this taxid 
		for taxid in taxid_to_assemble:
			taxid_search_list = [str(taxid)]
			taxid_search_list = taxid_search_list + ncbi.get_descendant_taxa(taxid, intermediate_nodes=True)
			list_of_reads_to_pull = []
			# this gets a list of every read that was assigned a given taxid 
			for a_line in open(base + '_assignments.txt'):
				a_line_list = a_line.split('	')
				if a_line_list[1]  in taxid_search_list:
					list_of_reads_to_pull.append(a_line_list[0])
			acc_num_list = []
			# this gets us all accession numbers that all these reads aligned to 
			for s_file in glob.glob(base + '*' + '.sam'):
				for line in open(s_file):
					sam_line_list = sam_line.split('	')
					if sam_line_list[0] in list_of_reads_to_pull and 'complete_genome' in sam_line_list[2]:
						acc_num_list.append(sam_line_list[2].split('.')[0])
			if len(acc_num_list) == 0:
				print('No complete genome reference found, not assembling taxid: ' + str(taxid))
				break
				
			# now we figure out the most common accession number that was assigned to this taxid
			most_common_acc_num = max(set(acc_num_list), key = acc_num_list.count)
			
			taxid_lineage = ncbi.get_lineage(taxid)
			# we walk up the taxonomy tree ASSEMBLY_NODE_OFFSET nodes and pull all reads that were taxonomically assigned at or below that node
			taxid_to_pull = taxid_lineage[ASSEMBLY_NODE_OFFSET]
			taxid_search_list = taxid_to_pull + ncbi.get_dsecendant_taxa(taxid_to_pull, intermediate_nodes = True)
			
			header_list = []
			seq_list = []
			g = open(base + '_' + taxid + '.fasta', 'w')
			# then we write all the reads that are at or below the taxid_to_pull variable and write them into a fasta file 
			for line in open(base + '_assignments.txt'):
				line_list = line.split('	')
				if int(line_list[1]) in taxid_search_list:
					g.write('>' + line_list[0] + '\n')
					g.write(line_list[2])
			g.close()
			
			# now we download the reference fasta file for the most common acession number 
			print('Searching NCBI for Accession number:' + most_common_acc_num + ' for taxid ' + str(taxid))
			record = Entrez.read(Entrez.esearch(db='nucleotide', term=most_common_acc_num))
			try:
				h2 = Entrez.efetch(db='nucleotide', id=record['IdList'][0], rettype='fasta', retmode='text')
			except:
				print(str(taxid) + ' did not return hits - not assembling')
				break
			
			# build some file names for the bowtie index and our reference fasta 
			ref_fasta = base + '_' + str(taxid) + '_ref.fasta'
			ref_db = base + '_' + str(taxid) + '_bwt_db'
			g = open(ref_fasta, 'w')
			g.write(h2.read())
			g.close()
			print('building bowtie2 index') 
			# build the bowtie2 index 
			subprocess_call('bowtie2-build ' + ref_fasta + ' ' + ref_db + ' > /dev/null 2>&1 ')
			print('Done with index build. Aligning...')
			# aling and output the sam file 
			subprocess_call('bowtie2 -x ' + ref_db + ' -@ ' + THREADS + ' -f -U ' + base + '_' + str(taxid) + '.fasta --no-unal > ' + base + '_' + str(taxid) + '.sam')
			subprocess_call('rm ' + ref_db)
				

base_start_time = timeit.default_timer()
read_to_taxids_map = {}
reads_seq_map = {}
#For every SAM file for a given sample, read in the SAM files.
file_start_time = timeit.default_timer()
sam_file = "e_coli_2.sam"
print(sam_file)
print('Reading in ' + sam_file)

#For every line in the SAM file
line_count = 0
for line in open(sam_file):
    line_count += 1
    #Skip the first three lines, which are header
    if line_count > 3:
        #For each read, pull the SAM information for that read.
        line_list = line.split('	')
        current_read = line_list[0]
        snap_assignment_of_current_read = line_list[2]
        sequence_of_current_read = line_list[9]
        
        #If read is unassigned, call it unassigned and throw it out.  Unassigned reads do not have an edit distance, assign it 100.
        if snap_assignment_of_current_read == '*':
            # higher edit distance than anything else makes sure this gets parsed out 
            current_read_taxid = [snap_assignment_of_current_read,100]
        else:
            #Pull the taxid and the edit distance from each line.
            current_read_taxid = [snap_assignment_of_current_read.split('#')[-1],
                int(line_list[12].split(':')[-1])]
        #Create map for each sample.
        #The key in each map is the read ID and the values are lists.
        #For every read, append a list of taxid and edit distance from each SAM file.
        if current_read in read_to_taxids_map:
            read_to_taxids_map[current_read].append(current_read_taxid)
        else: 
            # if this is the first time we've seen this read add the sequence to the list
            # and initialize the read -> taxid_list map  
            read_to_taxids_map[current_read] = [current_read_taxid]
            # also store the read and the sequence, this does need to be in a map 
            reads_seq_map[current_read] = sequence_of_current_read

file_runtime = str(timeit.default_timer() - file_start_time)
print('Reading in file ' + sam_file + ' took ' + file_runtime)

per_base_runtime = str(timeit.default_timer() - base_start_time)
print("e_coli_2" + ' took ' + per_base_runtime + ' in total to read')
final_assignment_counts = defaultdict(int)

#Now we've read all the reads for all the SAM files for one sample.  We are still within the sample For loop here.
#We have all the reads with all the taxids and edit distances that have been assigned.
print('Breaking ties for ' + "e_coli_2")
tie_break_start = timeit.default_timer()
# now we're done with the loop and we have a map with reads to list of taxids assigned to them
g = open("e_coli_2" + '_assignments.txt', 'w')
e = open("e_coli_2" + '_unassigned.txt','w')
unass_count = 0
taxid_to_read_set = {}

if "false" == "true":
    z = open("e_coli_2_recheck.txt", 'w')


for read_key in read_to_taxids_map.keys():
    # Now we need to assign only one taxid to each read, which means we need to run the tie-breaking function. 
    loaded_read = reads_seq_map[read_key]
    
    #Create a results list which is a taxid and a boolean, which answers whether I should re-BLAST this or not.
    r_list = tie_break(read_to_taxids_map[read_key])
    tax_assignment = r_list[0]
    
    # This will only ever be true if BLAST_CHECK is set to True
    if r_list[1]:
        z.write('>' + read_key + '\n' + loaded_read + '\n')
        
        
    
    #If the read is unassigned, write it to the unassigned file.
    if tax_assignment == '*':
        e.write('>' + read_key + '\n' + loaded_read + '\n')
        unass_count += 1
    # otherwise write it out to the read-by-read assignment file 
    else:
        g.write(read_key + '	' + str(tax_assignment) + '	' + loaded_read + '\n')
        # create a mapping of taxid -> unique reads.  Unique reads are defined as reads without the exact same sequence.  This can help in debugging.
        if "true" == "true":
            if str(tax_assignment) in taxid_to_read_set:
                taxid_to_read_set[str(tax_assignment)].add(loaded_read,)
            else:
                taxid_to_read_set[str(tax_assignment)] = set([loaded_read])
            
        if tax_assignment in final_assignment_counts:
            final_assignment_counts[tax_assignment] += 1
        else:
            final_assignment_counts[tax_assignment] = 1

g.close()
e.close()
if "false" == "true":
    z.close()
    subprocess_call('blastn -db input.3 -task blastn -query e_coli_2_recheck.txt -num_threads 20 -evalue 0.001 -outfmt "6 qseqid" -max_target_seqs 1 -max_hsps 1 > blast_check.txt')
    redo_taxid_list = []
    for line in open('blast_check.txt'):
        redo_taxid_list.append(line.split())
    n = open('new_assignments.txt', 'w')
    for line in open("e_coli_2" + '_assignments.txt'):
        ll = line.split('	')
        if ll[0] in redo_taxid_list:
            n.write(ll[0] + '	' + DB_TAXID + '	' + ll[2].strip() + '\n')
        else:
            n.write(line)
    n.close()
    for item in redo_taxid_list:
        final_assignment_counts[read_to_taxids_map[item]] += 1
        final_assignment_counts[DB_TAXID] += 1
    
        
        

#For each sample, we make a folder and for every taxid, we create a FASTA file that are named by their taxid.  We lose the read ID in this file.  #nicetohave would be hold the read ID here.
#Here we will write a FASTA of unique reads
if "true" == "true":
    subprocess_call('mkdir ' + "e_coli_2".split('.')[0])
    for id in taxid_to_read_set.keys():
        f = open("e_coli_2".split('.')[0] + '/' + str(id) + '_uniques.txt', 'w')
        count = 0
        for item in taxid_to_read_set[id]:
            f.write('>' + str(count) + '\n')
            f.write(item + '\n')
            count += 1
        f.close()
    
tie_break_time = str(timeit.default_timer() - tie_break_start)
print('Tie breaking ' + "e_coli_2" + ' took ' + tie_break_time)

#For each sample, write the Pavian output.
new_write_kraken("e_coli_2", final_assignment_counts, unass_count)

if "true" == "true":
    line_count = 0
    for log_line in open("e_coli_2.log", "rt"):
        line_count += 1
        if line_count == 4 or line_count == 5:
            final_assignment_counts[9606] += int(log_line.split()[0])
        if "false" == "true" and (line_count == 10 or line_count == 11):
            final_assignment_counts[9606] += int(log_line.split()[0])

new_write_kraken("e_coli_2" + '_with_host', final_assignment_counts, unass_count)


if "false" == "true":
    build_sams(sam_list)
